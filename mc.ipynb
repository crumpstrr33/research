{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce1150e-8879-47dc-8763-b7c7ddbd7e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "247bc06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from itertools import product\n",
    "import os, sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from plotly import graph_objects as go\n",
    "from scipy.stats import norm\n",
    "from sympy import lambdify, sympify\n",
    "from vegas import AdaptiveMap, Integrator\n",
    "\n",
    "# Local scripts found in 'scripts' directory, add to PYTHONPATH to import\n",
    "mc_path = os.path.abspath(os.path.join('scripts'))\n",
    "if mc_path not in sys.path:\n",
    "    sys.path.append(mc_path)\n",
    "from monte_carlo import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584ea81-8bfa-4daa-8b41-9c25fb5ddd36",
   "metadata": {},
   "source": [
    "## Variance\n",
    "\n",
    "## $$\\text{Var}\\big(X\\big)=E\\big[X^2\\big]+E\\big[X\\big]^2\\text{ and }\\text{Var}\\big(E[X]\\big)=\\frac{\\text{Var}\\big(X\\big)}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3321ec6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MC method:\n",
    "\n",
    "### $$\\displaystyle E[f]\\equiv E_U[f]=\\int_a^bf(x)\\ \\text{d}x=(b-a)\\int_a^b\\frac{f(x)}{b-a}\\ \\text{d}x\\approx\\frac{b-a}{N}\\sum_{i=1}^Nf(x_i)\\text{ where }x_i\\sim U[a,b].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668afe0-2dae-47eb-aff0-67ca56526a2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4b290",
   "metadata": {},
   "source": [
    "# Control Variate:\n",
    "\n",
    "### $$\\displaystyle\\int_a^bf(x)\\ \\text{d}x\\approx\\frac{b-a}{N}\\sum_{i=1}^N\\left(f(x_i)+cg(x_i)\\right)-cE[g(X)]\\text{ where }x_i\\sim U[a,b]\\text{ and }E[g(x)]=I_g\\text{ is known}.$$\n",
    "\n",
    "### $$\\text{The variance of is }f\\text{ minimized when }\\displaystyle c=-\\frac{\\text{Cov}(f,g)}{\\text{Var}(g)}=-\\frac{E[fg]-E[f]E[g]}{E[g^2]-E[g]^2}=\\frac{I_gE[f]-E[fg]}{E[g^2]-I_g^2}$$\n",
    "### $$\\text{The variance becomes: }\\text{Var}\\Big(f(X)+cg(X)\\Big)=\\left(1-\\text{Corr}(f,g)^2\\right)\\text{Var}(f)\\text{ where }\\text{Corr}(f,g)=\\frac{\\text{Cov}(f,g)}{\\sqrt{\\text{Var}(f)\\text{Var}(g)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac11ed3-b51b-46f3-a41c-4c2f56bc5cc2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9ad62-a23a-4c1f-9461-1d3ceeae5aaa",
   "metadata": {},
   "source": [
    "# Antithetic variates\n",
    "\n",
    "### $$\\displaystyle\\text{Let }f(X)=\\frac{f(X_1)+f(X_2)}{2}\\text{ where }x_1=\\text{CDF}^{-1}(u_1)\\sim X_1\\text{ and }x_2=\\text{CDF}^{-1}(1-u_1)\\sim X_2\\text{ where }u\\sim U[0,1]$$\n",
    "### then\n",
    "### $$\\text{Var}\\big(f(X)\\big)=\\frac{1}{4}\\bigg[\\text{Var}\\big(f(X_1)\\big)+\\text{Var}\\big(f(X_2)\\big)+2\\text{Cov}\\big(f(X_1),f(X_2)\\big)\\bigg]$$\n",
    "\n",
    "Since $X_1$ and $X_2$ have a negative correlation, their covariance will always be negative and thus $X$ will have a smaller variance than if $X_1$ and $X_2$ were uncorrelated (as it is in normal MC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35cfc94-af86-4940-b7fd-8783d889a676",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5bfcfb",
   "metadata": {},
   "source": [
    "# Wiki Example: $\\displaystyle f(x)=\\frac{1}{1+x}$ with $x\\sim U[0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82598f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "func = \"1 / (x + 1)\"\n",
    "cvfunc = \"x + 1\"\n",
    "soln = 3/2\n",
    "print(f\"Exact solution: {np.log(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f8720",
   "metadata": {},
   "source": [
    "## Normal MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5bd621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mcprint(MC(func, 0, 1, use_vegas=False), sigfigs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fc59b",
   "metadata": {},
   "source": [
    "## Control Variate MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d5829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mcprint(MCcv(func, cvfunc, soln, cN=int(1e6), N=int(1e6), use_vegas=False), sigfigs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb5bda6-d17f-454a-89d9-de4d8a2f51c5",
   "metadata": {},
   "source": [
    "The correlation coefficient is between -1 and 1. We want to maximize it with the choice of the variate function. Looking at the correlation between $\\displaystyle\\frac{1}{1+x}$ and $x+1$ to see variance reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097d13e-5fcc-4adf-8a62-150cc2c1f57e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{100 * corr(func, cvfunc, expval2=soln)**2:.3f}% reduction in variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc44b27e-1ce8-404e-acc2-dcbba052abd0",
   "metadata": {},
   "source": [
    "## Antithetic Variate MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b3f393-5288-4f43-96ed-eddff963f5e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mcprint(MCav(func), sigfigs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566148e9-304c-49f8-a6e2-e9aeb66722c0",
   "metadata": {},
   "source": [
    "This works if $\\text{Cov}(Y_1,Y_2)<0$. Here $\\displaystyle Y_1=\\frac{1}{1+X}$ and $\\displaystyle Y_1=\\frac{1}{1+(1-X)}=\\frac{1}{2-X}$. So,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef0099-f92f-420a-aa5c-06a7b5ea21d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y1 = str(sympify(func))\n",
    "Y2 = str(sympify(Y1.replace('x', '(1-x)')))\n",
    "covy1y2 = cov(Y1, Y2)\n",
    "print(f\"The covariance is {covy1y2:.3f}. It is {'less' if covy1y2 < 0 else 'greater'} than zero.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a416a-e7d5-4be9-8d3a-7aceb9190d09",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8330a9-7e90-4c35-804d-945020eb250f",
   "metadata": {},
   "source": [
    "## Combining Control and Antithetic Variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb960ad-bccf-49a1-a17d-8740a93b28b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAKE THIS ACTUALLY WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899473b3-553e-4ef2-a043-e9108b664af9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42287990-952f-48de-ac4b-0ffeaad45b66",
   "metadata": {},
   "source": [
    "# MC From Distribution With Known Inverse CDF\n",
    "\n",
    "### $$\\int_a^bf(x)g(x)\\text{d}x=\\frac{1}{N}\\sum_{i=1}^Nf(x_i)\\text{ where }x_i\\sim g(X)$$\n",
    "\n",
    "$g(X)$ is the PDF of $f(X)$. Samples can be drawn from that distribution using a uniform distribution. If $U[0,1]$ is a uniform distribution, then $g(X)=\\text{CDF}_{g}^{-1}(U)$. So using the inverse CDF, we can make draws using a uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b09745-93d5-447b-b5d3-e954761ca0ab",
   "metadata": {},
   "source": [
    "## Your Very Own PDF Checker!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80138ab-43b6-4d35-b4ba-8256f6721d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xi, xf = -5, 5\n",
    "\n",
    "xs = np.linspace(xi, xf, 1000)\n",
    "dist = 'normal'\n",
    "kwargs = {'mu': 0, 'sigma': 2}\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=inv_cdfs[dist](**kwargs, N=100_000), histnorm='probability density', name='Inv CDF'))\n",
    "fig.add_trace(go.Scatter(x=xs, y=pdfs[dist](xs, **kwargs), mode='lines', name='Exact PDF'))\n",
    "fig.update_xaxes(range=(xi, xf))\n",
    "fig.update_layout(title=f\"Distribution: {dist.title()}\", height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47914de-c36a-470f-bafd-75e7064ee213",
   "metadata": {},
   "source": [
    "## And checking if the variance found from MC is the actual variance (using a normal distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95a212-a966-47d2-905a-7229617f278f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_var(mc_func, Ntrials, Nxs, Nstds, mc_func_kwargs):\n",
    "    # Get distributions for expectation value and variance\n",
    "    valvars = np.array([mc_func(**mc_func_kwargs) for _ in range(Ntrials)])\n",
    "    vals, varis = valvars[:, 0], valvars[:, 1]\n",
    "\n",
    "    # Get mean values for expectation value and standard deviation\n",
    "    avgval = np.mean(vals)\n",
    "    avgstd = np.mean(np.sqrt(varis))\n",
    "    fitval, fitstd = norm.fit(vals, loc=avgval, scale=avgstd)\n",
    "\n",
    "    mc_color = 'red'\n",
    "    fit_color = 'green'\n",
    "    xs = np.linspace(avgval - Nstds*avgstd, avgval + Nstds*avgstd, Nxs)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x=vals, histnorm='probability density', name='MC Values'))\n",
    "    fig.add_trace(go.Scatter(x=xs, y=norm.pdf(xs, loc=fitval, scale=fitstd), line=dict(color=fit_color), name='Fitted Gaussian'))\n",
    "    fig.add_trace(go.Scatter(x=xs, y=norm.pdf(xs, loc=avgval, scale=avgstd), line=dict(color=mc_color), name='MC Gaussian'))\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        xaxis_title='Estimation of expectation value'\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        x=0.99, y=0.98,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        showarrow=False,\n",
    "        text=f'Mean: {fitval:.6f} | St Dev: {fitstd:.6f}',\n",
    "        font_size=20,\n",
    "        bgcolor=fit_color,\n",
    "        bordercolor='black',\n",
    "        borderwidth=1\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        x=0.99, y=0.90,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        showarrow=False,\n",
    "        text=f'Mean: {avgval:.6f} | St Dev: {avgstd:.6f}',\n",
    "        font_size=20,\n",
    "        bgcolor=mc_color,\n",
    "        bordercolor='black',\n",
    "        borderwidth=1\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aced79-8d99-447e-b784-a78ba33e6829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# e.g.\n",
    "check_var(MCav, 500, 100000, 8, dict(func='1 / (x + 1)', N=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b24a9-a2ff-4a6e-8285-d02e122b084e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa82edb-c241-46d5-838c-97a6e0e41f0f",
   "metadata": {},
   "source": [
    "# Vegas Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440f6ef-ad63-460c-a5ef-2730dcba1052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vegas \n",
    "def f(x):\n",
    "    dx2 = 0\n",
    "    for d in range(4):\n",
    "        dx2 += (x[d] - 0.5) ** 2\n",
    "    return np.exp(-dx2 * 100.) * 1013.2118364296088\n",
    "\n",
    "integ = Integrator([[-1, 1], [0, 1], [0, 1], [0, 1]])\n",
    "\n",
    "result = integ(f, nitn=10, neval=1000)\n",
    "print(result.summary())\n",
    "print('result = %s    Q = %.2f' % (result, result.Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13295303-f5d7-4111-8f3b-0f9d46f50454",
   "metadata": {},
   "source": [
    "- `nitn` is number of iterations. Vegas tries to flatten the integrand via a transformation and each iteration, it uses info from the previous to optimize the transformation.\n",
    "- `neval` is the maximum number of evaluations per iteration. \n",
    "- The weight average is weighted by the inverse variance so the first few iterations (which are very inaccurate) add very little to the weight average.\n",
    "- $Q$ called $p$-value is the probability that a larger $\\chi^2$ could result from random Gaussian fluctuations.\n",
    "- A small value, $Q<0.1$, says that the estimates of the integral from different iterations do not agree with each other within error.\n",
    "    - So `neval` must be increased to trust the error estimates, i.e. `neval` isn't guarenteeing Gaussian behavior.\\\\\n",
    "- Computing cost is roughly proportional to `nitn`$*$`neval`.\n",
    "- Using too many iterations can be bad. Use no more than 10-20 iterations beyond where Vegas has fully adapted.\n",
    "- Systematic error vanishes by at least 1/`neval`.\n",
    "- Increasing `neval` will guarantee a decrease in both statistical and systematic uncertainties.\n",
    "- Increasing `nitn` will gaurantee to eventually give the wrong answer.\n",
    "    - Statistical error falls like sqrt(1/`nitn`).\n",
    "    - Systematic error isn't affected by `nitn`.\n",
    "    - So eventually systematic errors become significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e719a97-408d-482b-9029-0b2edf512372",
   "metadata": {},
   "source": [
    "### Sometimes it is useful to throw away early iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89604499-2a29-4932-93a5-5505f020bbf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "integ = vegas.Integrator([[-2, 2], [0, 2], [0, 2], [0., 2]])\n",
    "result = integ(f, nitn=10, neval=1000)\n",
    "print(result.summary())\n",
    "print(f\"Result={result}, Q={result.Q:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c20a7-69ec-452f-a3c6-9d4e844851d3",
   "metadata": {},
   "source": [
    "A $Q=0$ means our result is completely unreliable. Sometimes Vegas finds the peak and we get a reasonable answer but most of the time we don't. So we throw away the first few inaccurate iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583788bd-21d7-44f6-b9e0-21b5c0b9a006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adapt to f, discard results\n",
    "integ(f, nitn=10, neval=10000)\n",
    "\n",
    "# `integ` has adapted to f, keep results\n",
    "result = integ(f, nitn=10, neval=10000)\n",
    "print(result.summary())\n",
    "print(f\"Result={result}, Q={result.Q:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4bd843-2dd7-42b1-a02a-2ac80ce472fd",
   "metadata": {},
   "source": [
    "Can use non-rectangular bounds by using `if/else` statements in function. That is, if the value is outside your bounds, then return `0`. Choice of bounds for the `Integrator` object is important otherwise Vegas will spend a lot of time where the integral is zero. This is more pronounced the higher the dimension is.\n",
    "- `alpha` controls how quickly Vegas adapts. It defaults to 0.5. If there are persistent, large fluctuations in the size of the per-iteration errors, then `alpha` may need to be reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d2e19-b7cf-41ee-9c8c-9a1da377bfd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T17:34:47.489048Z",
     "iopub.status.busy": "2022-11-02T17:34:47.488899Z",
     "iopub.status.idle": "2022-11-02T17:34:47.502140Z",
     "shell.execute_reply": "2022-11-02T17:34:47.501606Z",
     "shell.execute_reply.started": "2022-11-02T17:34:47.489035Z"
    }
   },
   "source": [
    "### Batch Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a073bf4-79e9-4049-97d4-c12fa6f2bdb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    dx2 = 0\n",
    "    for d in range(4):\n",
    "        dx2 += (x[d] - 0.5) ** 2\n",
    "    return np.exp(-dx2 * 100.) * 1013.2118364296088\n",
    "\n",
    "@vegas.batchintegrand\n",
    "def f_batch(x):\n",
    "    dim = x.shape[1]\n",
    "    norm = 1013.2118364296088 ** (dim / 4)\n",
    "    dx2 = 0\n",
    "    for d in range(4):\n",
    "        dx2 += (x[:, d] - 0.5) ** 2\n",
    "    return np.exp(-dx2 * 100.) * norm\n",
    "\n",
    "integ = vegas.Integrator(4 * [[0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd6f8e-566e-4bf7-b147-a67662121388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit integ(f, nitn=10, neval=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba7e04-70b1-4c3c-bc75-93bacaf22937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit integ(f_batch, nitn=10, neval=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd23eb-c766-42c8-a850-83bff1f5cdc8",
   "metadata": {},
   "source": [
    "Note a 10x speedup. Batch integration can also be coded with a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085f3b4-23bc-4cae-a3f5-552b947e830b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class f_batch(vegas.BatchIntegrand):\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.norm = 1013.2118364296088 ** (dim / 4.)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        dx2 = 0.0\n",
    "        for d in range(self.dim):\n",
    "            dx2 += (x[:, d] - 0.5) ** 2\n",
    "        return np.exp(-100. * dx2) * self.norm\n",
    "\n",
    "f = f_batch(dim=4)\n",
    "integ = vegas.Integrator(f.dim * [[0, 1]])\n",
    "\n",
    "integ(f, nitn=10, neval=2e5)\n",
    "result = integ(f, nitn=10, neval=2e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee0646-6183-442b-ab9a-79aef1a7b550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-02T14:43:01.270350Z",
     "iopub.status.busy": "2022-11-02T14:43:01.269492Z",
     "iopub.status.idle": "2022-11-02T14:43:01.292063Z",
     "shell.execute_reply": "2022-11-02T14:43:01.291219Z",
     "shell.execute_reply.started": "2022-11-02T14:43:01.270304Z"
    }
   },
   "source": [
    "# Testing Grounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647de76-126b-4e8d-9b62-c12e7eae0f9d",
   "metadata": {},
   "source": [
    "For an integral\n",
    "\\begin{align}\n",
    "    I=\\int_a^bf(x)\\ \\text{d}x\n",
    "\\end{align}\n",
    "rewrite it over a new variable $y$\n",
    "\\begin{align}\n",
    "    I=\\int_0^1J(y)f(x(y))\\ \\text{d}y\\qquad\\text{where}\\qquad J(y)=\\frac{\\partial{x}}{\\partial{y}}\n",
    "\\end{align}\n",
    "is the Jacobian of the transformation. The Monte Carlo estimation is then\n",
    "\\begin{align}\n",
    "    I\\approx\\frac{1}{N}\\sum_{i=1}^NJ(y_i)f(x(y_i))\n",
    "\\end{align}\n",
    "where the $y_i$'s are drawn from a uniform distribution $U[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "0fd3aab0-1033-4ca5-9d8a-3485c90d4782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6703166240981648"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My attempt using the docs for Adaptive map\n",
    "def f(x):\n",
    "    return 1 / (1 + x)\n",
    "\n",
    "# `ninc` is the number of increments between 0 and 1\n",
    "admap = AdaptiveMap([[0, 1]], ninc=100_000)\n",
    "\n",
    "# Initial uniformly random choices\n",
    "ny = 100\n",
    "y = np.random.uniform(0, 1, (ny, 1))\n",
    "\n",
    "# Initializing arrays\n",
    "x = np.empty(y.shape, float)\n",
    "jac = np.empty(y.shape[0], float)\n",
    "f2 = np.empty(y.shape[0], float)\n",
    "\n",
    "# Do 100 iterations of adaptation\n",
    "for itn in range(100):\n",
    "    admap.map(y, x, jac)\n",
    "\n",
    "    # Don't understand why it's squared...\n",
    "    for j in range(ny):\n",
    "        f2[j] = (jac[j] * f(x[j])) ** 2\n",
    "\n",
    "    admap.add_training_data(y, f2)\n",
    "    admap.adapt(alpha=0.5)\n",
    "\n",
    "# Do the sampling, not very accurate\n",
    "n = 10_000\n",
    "f(np.random.choice(admap.extract_grid()[0], n)).sum() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "60fcc7f8-b977-4767-909f-302084c84bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = 0.7207939516487306\n"
     ]
    }
   ],
   "source": [
    "# Using Prasanth's code, you get the wrong answer only if you adapt first which I don't understand\n",
    "integration_region = [[0, 1]]\n",
    "def importance_sampling(f, sampling_map, neval, rng):\n",
    "    y = rng.uniform(low=0.0, high=1.0, size=(neval, 1))\n",
    "    x = sampling_map(y)\n",
    "\n",
    "    return sampling_map.jac(y) * f(x)\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "integrator = Integrator(integration_region)\n",
    "# Adapting integrator\n",
    "integrator(f, nitn=20, neval=1000)\n",
    "weight_values = importance_sampling(\n",
    "    f=f,\n",
    "    sampling_map=integrator.map,\n",
    "    neval=10_000,\n",
    "    rng=rng\n",
    ")\n",
    "print(f\"mean = {np.mean(weight_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "e2d8ce2e-335d-4192-888a-4e157056b8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itn   integral        wgt average     chi2/dof        Q\n",
      "-------------------------------------------------------\n",
      "  1   0.693115(39)    0.693115(39)        0.00     1.00\n",
      "  2   0.693115(37)    0.693115(27)        0.00     0.99\n",
      "  3   0.693110(38)    0.693113(22)        0.01     0.99\n",
      "  4   0.693168(37)    0.693127(19)        0.55     0.65\n",
      "  5   0.693105(36)    0.693123(17)        0.48     0.75\n",
      "  6   0.693208(37)    0.693137(15)        1.28     0.27\n",
      "  7   0.693173(37)    0.693142(14)        1.20     0.30\n",
      "  8   0.693121(37)    0.693140(13)        1.07     0.38\n",
      "  9   0.693070(38)    0.693132(12)        1.31     0.23\n",
      " 10   0.693194(36)    0.693139(12)        1.45     0.16\n",
      "\n",
      "I = 0.693139(12)\n"
     ]
    }
   ],
   "source": [
    "# Vegas's black box way\n",
    "intr = Integrator([[0, 1]])\n",
    "result = intr(f)\n",
    "print(result.summary())\n",
    "print(f\"I = {result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6946de-c248-4281-a886-9e80172ed238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "395697348ad52bb1451b0fdaeb78476e5b64aef5b2e50285cbda90e166270d6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
